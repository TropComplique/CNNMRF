{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from model import Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SIZE = (600, 400)\n",
    "\n",
    "CONTENT = Image.open('dog.jpg').resize(FINAL_SIZE, Image.LANCZOS)\n",
    "STYLE = Image.open('Vasily Kandinsky Small Worlds I.jpg')\n",
    "print('size of the style image', STYLE.size)\n",
    "\n",
    "ANGLES = [-45, 0, 45]\n",
    "SCALES = [0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(images):\n",
    "    \"\"\"\n",
    "    Shows a list of images.\n",
    "    Images can be of different sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    width = max(i.size[0] for i in images)\n",
    "    height = sum(i.size[1] for i in images)\n",
    "    background = Image.new('RGB', (width, height), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(background, 'RGB')\n",
    "\n",
    "    offset = 0\n",
    "    for i in images:\n",
    "        _, h = i.size\n",
    "        background.paste(i, (0, offset))\n",
    "        offset += h\n",
    "\n",
    "    return background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = STYLE.size\n",
    "\n",
    "STYLES = []\n",
    "for a in ANGLES:\n",
    "    for s in SCALES:\n",
    "        w, h = int(width * s), int(height * s)\n",
    "        resized = STYLE.resize((w, h), Image.LANCZOS)\n",
    "        rotated = resized.rotate(a, Image.BICUBIC)\n",
    "        box = (0.2 * w, 0.2 * h, 0.8 * w, 0.8 * h)\n",
    "        cropped = rotated.crop(box)# if a != 0 else rotated\n",
    "        STYLES.append(cropped)\n",
    "\n",
    "show(STYLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def synthesize1(content, initial):\n",
    "    \n",
    "    objective = Loss(content, styles=STYLES, initial=initial).to(DEVICE)\n",
    "    params = filter(lambda x: x.requires_grad, objective.parameters())\n",
    "\n",
    "    NUM_STEPS = 10000\n",
    "    optimizer = optim.Adam(params, lr=1e-4)\n",
    "\n",
    "    text = 'i:{0}, total:{1:.2f}, content:{2:.3f}, style:{3:.6f}, tv:{4:.4f}'\n",
    "    for i in range(1, NUM_STEPS + 1):\n",
    "\n",
    "        objective.x.data.clamp_(0, 1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        content_loss, style_loss, tv_loss = objective()\n",
    "        total_loss = 2 * content_loss + 1 * style_loss + 6000 * tv_loss\n",
    "        total_loss.backward()\n",
    "    \n",
    "        if i % 100 == 0 or i == 1:\n",
    "            print(text.format(i, total_loss.item(), content_loss.item(), style_loss.item(), tv_loss.item()))\n",
    "        optimizer.step()\n",
    "        \n",
    "    result = 255 * objective.x.clamp(0, 1).detach().permute(0, 2, 3, 1)[0].cpu().numpy()\n",
    "    result = Image.fromarray(result.astype('uint8'))\n",
    "\n",
    "    del objective\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_upsamplings = 2  # non negative integer\n",
    "\n",
    "width, height = CONTENT.size\n",
    "s = 2**num_upsamplings\n",
    "size = (width // s, height // s)\n",
    "print('synthesizing image of size', size)\n",
    "x = synthesize1(CONTENT.resize(size, Image.LANCZOS), initial=CONTENT.resize(size, Image.LANCZOS))\n",
    "\n",
    "results = [x]\n",
    "for _ in range(num_upsamplings):\n",
    "\n",
    "    width, height = x.size\n",
    "    size = (width * 2, height * 2)\n",
    "    print('\\nsynthesizing image of size', size)\n",
    "\n",
    "    initial = x.resize(size, Image.LANCZOS)\n",
    "    x = synthesize1(CONTENT.resize(size, Image.LANCZOS), initial)\n",
    "    results.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize with L-BFGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize2(content, initial):\n",
    "    \n",
    "    objective = Loss(content, styles=STYLES, initial=initial).to(DEVICE)\n",
    "    params = filter(lambda x: x.requires_grad, objective.parameters())\n",
    "\n",
    "    NUM_STEPS = 3000\n",
    "    optimizer = optim.LBFGS(\n",
    "        params=params, lr=0.1, max_iter=NUM_STEPS, \n",
    "        tolerance_grad=-1, tolerance_change=-1\n",
    "    )\n",
    "    \n",
    "    i = [1]\n",
    "    text = 'i:{0}, total:{1:.2f}, content:{2:.3f}, style:{3:.6f}, tv:{4:.4f}'\n",
    "    def closure():\n",
    "\n",
    "        objective.x.data.clamp_(0, 1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        content_loss, style_loss, tv_loss = objective()\n",
    "        total_loss = 1 * content_loss + 2 * style_loss + 7000 * tv_loss\n",
    "        total_loss.backward()\n",
    "        \n",
    "        if i[0] % 100 == 0 or i[0] == 1:\n",
    "            print(text.format(i[0], total_loss.item(), content_loss.item(), style_loss.item(), tv_loss.item()))\n",
    "        \n",
    "        i[0] += 1\n",
    "        return total_loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    result = 255 * objective.x.clamp(0, 1).detach().permute(0, 2, 3, 1)[0].cpu().numpy()\n",
    "    result = Image.fromarray(result.astype('uint8'))\n",
    "\n",
    "    del objective\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_upsamplings = 2  # non negative integer\n",
    "\n",
    "width, height = CONTENT.size\n",
    "s = 2**num_upsamplings\n",
    "size = (width // s, height // s)\n",
    "print('synthesizing image of size', size)\n",
    "x = synthesize2(CONTENT.resize(size, Image.LANCZOS), initial=CONTENT.resize(size, Image.LANCZOS))\n",
    "\n",
    "results = [x]\n",
    "for _ in range(num_upsamplings):\n",
    "\n",
    "    width, height = x.size\n",
    "    size = (width * 2, height * 2)\n",
    "    print('\\nsynthesizing image of size', size)\n",
    "\n",
    "    initial = x.resize(size, Image.LANCZOS)\n",
    "    x = synthesize2(CONTENT.resize(size, Image.LANCZOS), initial)\n",
    "    results.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
