{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from losses import PerceptualLoss, TotalVariationLoss, MarkovRandomFieldLoss, Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SIZE = (600, 400)\n",
    "\n",
    "CONTENT = Image.open('dog.jpg').resize(FINAL_SIZE, Image.LANCZOS)\n",
    "STYLE = Image.open('cat.jpg')\n",
    "\n",
    "ANGLES = [-45, 0, 45]\n",
    "SCALES = [0.8, 1.0, 1.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = STYLE.size\n",
    "\n",
    "STYLES = []\n",
    "for a in ANGLES:\n",
    "    for s in SCALES:\n",
    "        w, h = int(width * s), int(height * s)\n",
    "        resized = STYLE.resize((w, h), Image.LANCZOS)\n",
    "        rotated = resized.rotate(a, Image.BICUBIC)\n",
    "        box = (0.2 * w, 0.2 * h, 0.8 * w, 0.8 * h)\n",
    "        cropped = rotated.crop(box) if a != 0 else rotated\n",
    "        STYLES.append(cropped)\n",
    "\n",
    "width = max(s.size[0] for s in STYLES)\n",
    "height = sum(s.size[1] for s in STYLES)\n",
    "background = Image.new('RGB', (width, height), (255, 255, 255))\n",
    "draw = ImageDraw.Draw(background, 'RGB')\n",
    "\n",
    "offset = 0\n",
    "for s in STYLES:\n",
    "    _, h = s.size\n",
    "    background.paste(s, (0, offset))\n",
    "    offset += h\n",
    "    \n",
    "background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x):\n",
    "    x = np.array(x)\n",
    "    x = torch.FloatTensor(x)\n",
    "    # convert to the NCHW format and the [0, 1] range\n",
    "    return x.permute(2, 0, 1).unsqueeze(0)/255.0\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "\n",
    "    def __init__(self, content, styles, initial=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            content: an instance of PIL image.\n",
    "            styles: a list of PIL images.\n",
    "            initial: an instance of PIL image or None.\n",
    "        \"\"\"\n",
    "        super(Loss, self).__init__()\n",
    "        \n",
    "        if initial is None:\n",
    "            mean, std = 0.5, 1e-3\n",
    "            w, h = content.size\n",
    "            initial = mean + std * torch.randn(1, 3, h, w)\n",
    "        else:\n",
    "            assert initial.size == content.size\n",
    "            initial = to_tensor(initial)\n",
    "\n",
    "        # images\n",
    "        content = to_tensor(content)\n",
    "        styles = [to_tensor(s) for s in styles]\n",
    "        self.x = nn.Parameter(data=initial, requires_grad=True)\n",
    "\n",
    "        # features\n",
    "        self.vgg = Extractor()\n",
    "        cf = self.vgg(content)\n",
    "        sf = [self.vgg(s) for s in styles]\n",
    "\n",
    "        # names of features to use\n",
    "        content_layers = ['relu4_2']\n",
    "        style_layers = ['relu3_1', 'relu4_1']\n",
    "        num_styles = len(styles)\n",
    "\n",
    "        # create losses\n",
    "        self.content = nn.ModuleDict({\n",
    "            n: PerceptualLoss(cf[n]) \n",
    "            for n in content_layers\n",
    "        })\n",
    "        self.style = nn.ModuleDict({\n",
    "            n: MarkovRandomFieldLoss(\n",
    "                [sf[i][n] for i in range(num_styles)],\n",
    "                size=3, stride=1, threshold=1e-2\n",
    "            ) \n",
    "            for n in style_layers\n",
    "        })\n",
    "        self.tv = TotalVariationLoss()\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        f = self.vgg(self.x)\n",
    "        content_loss = torch.tensor(0.0, device=self.x.device)\n",
    "        style_loss = torch.tensor(0.0, device=self.x.device)\n",
    "        tv_loss = self.tv(self.x)\n",
    "            \n",
    "        for n, m in self.content.items():\n",
    "            content_loss += m(f[n])\n",
    "            \n",
    "        for n, m in self.style.items():\n",
    "            style_loss += m(f[n])\n",
    "    \n",
    "        return content_loss, style_loss, tv_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def synthesize(content, initial):\n",
    "    \n",
    "    objective = Loss(content, styles=STYLES, initial=initial).to(DEVICE)\n",
    "    params = filter(lambda x: x.requires_grad, objective.parameters())\n",
    "\n",
    "    NUM_STEPS = 500\n",
    "    optimizer = optim.Adam(params, lr=1e-2)\n",
    "\n",
    "    text = 'i:{0},total:{1:.2f},content:{2:.3f},style:{3:.6f},tv:{4:.4f}'\n",
    "    for i in range(NUM_STEPS):\n",
    "\n",
    "        objective.x.data.clamp_(0, 1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        content_loss, style_loss, tv_loss = objective()\n",
    "        total_loss = 2 * content_loss + style_loss + 1000 * tv_loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        print(text.format(i, total_loss.item(), content_loss.item(), style_loss.item(), tv_loss.item()))\n",
    "        optimizer.step()\n",
    "        \n",
    "    result = 255 * objective.x.clamp(0, 1).detach().permute(0, 2, 3, 1)[0].cpu().numpy()\n",
    "    return Image.fromarray(result.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 8\n",
    "assert s % 2 == 0\n",
    "num_upsamplings = int(np.log2(s))\n",
    "\n",
    "w, h = CONTENT.size\n",
    "x = synthesize(CONTENT.resize((w // s, h // s)), initial=None)\n",
    "\n",
    "results = [x]\n",
    "for _ in range(num_upsamplings):\n",
    "\n",
    "    w, h = x.size\n",
    "    initial = x.resize((w * 2, h * 2))\n",
    "\n",
    "    x = synthesize(CONTENT.resize((w * 2, h * 2)), initial)\n",
    "    results.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 255 * objective.x.clamp(0, 1).detach().permute(0, 2, 3, 1)[0].cpu().numpy()\n",
    "Image.fromarray(result.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = objective.x.detach().permute(0, 2, 3, 1)[0].cpu().numpy()\n",
    "result = 255*(result - result.min())/(result.max() - result.min())\n",
    "Image.fromarray(result.astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize with L-BFGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "objective = Loss(content, style).to(DEVICE)\n",
    "params = filter(lambda x: x.requires_grad, objective.parameters())\n",
    "\n",
    "optimizer = optim.LBFGS(\n",
    "    params=params, lr=0.1, max_iter=300, \n",
    "    tolerance_grad=-1, tolerance_change=-1\n",
    ")\n",
    "\n",
    "text = 'total:{0:.2f},content:{1:.3f},style:{2:.6f},tv:{3:.4f}'\n",
    "def closure():\n",
    "\n",
    "    objective.x.data.clamp_(0, 1)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    content_loss, style_loss, tv_loss = objective()\n",
    "    total_loss = content_loss + 100 * style_loss + 1000 * tv_loss\n",
    "    total_loss.backward()\n",
    "\n",
    "    print(text.format(total_loss.item(), content_loss.item(), style_loss.item(), tv_loss.item()))\n",
    "    return total_loss\n",
    "\n",
    "optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 255 * objective.x.clamp(0, 1).detach().permute(0, 2, 3, 1)[0].cpu().numpy()\n",
    "Image.fromarray(result.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = objective.x.detach().permute(0, 2, 3, 1)[0].cpu().numpy()\n",
    "result = 255*(result - result.min())/(result.max() - result.min())\n",
    "Image.fromarray(result.astype('uint8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
